@inproceedings{Zhang:2025:PIE,
author = {Zhang, Diyang and Wang, Zhendong and Liu, Zegao and Pei, Xinming and Xu, Weiwei and Wang, Huamin},
title = {Physics-inspired Estimation of Optimal Cloth Mesh Resolution},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730619},
doi = {10.1145/3721238.3730619},
abstract = {In this paper, we tackle an important yet often overlooked question: What is the optimal mesh resolution for cloth simulation, without relying on preliminary simulations? The optimal resolution should be sufficient to capture fine details of all potential wrinkles, while avoiding an unnecessarily high resolution that wastes computational time and memory on excessive vertices. This challenge stems from the complex nature of wrinkle distribution, which varies spatially, temporally, and anisotropically across different orientations. To address this, we propose a method to estimate the optimal cloth mesh resolution, based on two key factors: material stiffness and boundary conditions.To determine the influence of material stiffness on wrinkle wavelength and amplitude, we apply the experimental theory presented by Cerda and Mahadevan [2003] to calculate the optimal mesh resolution for cloth fabrics. Similarly, for boundary conditions influencing local wrinkle formation, we use the same scaling law to determine the source resolution for stationary boundary conditions introduced by garment-making techniques such as shirring, folding, stitching, and down-filling, as well as predicted areas accounting for dynamic wrinkles introduced by collision compression caused by human motions. To ensure a smooth transition between different source resolutions, we apply another experimental theory from [Vandeparre et al. 2011] to compute the transition distance. A mesh sizing map is introduced to facilitate smooth transitions, ensuring precision in critical areas while maintaining efficiency in less important regions. Based on these sizing maps, triangular meshes with optimal resolution distribution are generated using Poisson sampling and Delaunay triangulation. The resulting method can not only enhance the realism and precision of cloth simulations but also support diverse application scenarios, making it a versatile solution for complex garment design.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
articleno = {23},
numpages = {11},
keywords = {mesh resolution, wrinkle wavelength, material stiffness},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH Conference Papers '25}
}

@inproceedings{He:2025:ATS,
author = {He, Chengzhu and Wang, Zhendong and Meng, Zhaorui and Yao, Junfeng and Guo, Shihui and Wang, Huamin},
title = {Automated Task Scheduling for Cloth and Deformable Body Simulations in Heterogeneous Computing Environments},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730625},
doi = {10.1145/3721238.3730625},
abstract = {The concept of the Internet of Things (IoT) has driven the development of system-on-a-chip (SoC) technology for embedded and mobile systems, which may define the future of next-generation computation. In SoC devices, efficient cloth and deformable body simulations require parallelized, heterogeneous computation across multiple processing units. The key challenge in heterogeneous computation lies in task distribution, which must account for varying inter-task dependencies and communication costs. This paper proposes a novel framework for automated task scheduling to optimize simulation performance by minimizing communication overhead and aligning tasks with the specific strengths of each device. To achieve this, we introduce an efficient scheduling method based on the Heterogeneous Earliest Finish Time (HEFT) algorithm, adapted for hybrid systems. We model simulation tasks—such as those in iterative methods like Jacobi and Gauss-Seidel—as a Directed Acyclic Graph (DAG). To maximize the parallelism of nonlinear Gauss-Seidel simulation tasks, we present an innovative asynchronous Gauss-Seidel method with specialized data synchronization across units. Additionally, we employ task merging and tailored task-sorting strategies for Gauss-Seidel tasks to achieve an optimal balance between convergence and efficiency. We validate the effectiveness of our framework across various simulations, including XPBD, vertex block descent, and second-order stencil descent, using Apple M-series processors with both CPU and GPU cores. By maximizing computational efficiency and reducing processing times, our method achieves superior simulation frame rates compared to approaches that rely on individual devices in isolation. The source code with hybrid Metal/C++ implementation is available at https://github.com/ChengzhuUwU/libAtsSim.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
articleno = {24},
numpages = {11},
keywords = {task scheduling, heterogeneous computing, asynchronous parallelism},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH Conference Papers '25}
}

@inproceedings{Guo:2025:FPB,
author = {Guo, Dewen and Wang, Zhendong and Liu, Zegao and Li, Sheng and Wang, Guoping and Yang, Yin and Wang, Huamin},
title = {Fast Physics-Based Modeling of Knots and Ties using Templates},
year = {2025},
isbn = {9798400715402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3721238.3730622},
doi = {10.1145/3721238.3730622},
abstract = {Knots and ties are captivating elements of digital garments and accessories, but they have been notoriously challenging and computationally expensive to model manually. In this paper, we propose a physics-based modeling system for knots and ties using templates. The primary challenge lies in transforming cloth pieces into desired knot and tie configurations in a controllable, penetration-free manner, particularly when interacting with surrounding meshes. To address this, we introduce a pipe-like parametric knot template representation, defined by a B\'{e}zier curve as its medial axis and an adaptively adjustable radius for enhanced flexibility and variation. This representation enables customizable knot sizes, shapes, and styles while ensuring intersection-free results through robust collision detection techniques. Using the defined knot template, we present a mapping and penetration-free initialization method to transform selected cloth regions from UV space into the initial 3D knot shape. We further enable quasistatic simulation of knots and their surrounding meshes through a fast and reliable collision handling and simulation scheme. Our experiments demonstrate the system’s effectiveness and efficiency in modeling a wide range of digital knots and ties with diverse styles and shapes, including configurations that were previously impractical to create manually.},
booktitle = {Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers},
articleno = {43},
numpages = {9},
keywords = {cloth simulation, intersection-free, physics-based modeling},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH Conference Papers '25}
}

@article{Lan:2025:JGS,
author = {Lan, Lei and Lu, Zixuan and Yuan, Chun and Xu, Weiwei and Su, Hao and Wang, Huamin and Jiang, Chenfanfu and Yang, Yin},
title = {{JGS2}: Near Second-order Converging {Jacobi/Gauss-Seidel} for {GPU} Elastodynamics},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3731183},
doi = {10.1145/3731183},
abstract = {In parallel simulation, convergence and parallelism are often seen as inherently conflicting objectives. Improved parallelism typically entails lighter local computation and weaker coupling, which unavoidably slow the global convergence. This paper presents a novel GPU algorithm that achieves convergence rates comparable to fullspace Newton's method while maintaining good parallelizability just like the Jacobi method. Our approach is built on a key insight into the phenomenon of overshoot. Overshoot occurs when a local solver aggressively minimizes its local energy without accounting for the global context, resulting in a local update that undermines global convergence. To address this, we derive a theoretically second-order optimal solution to mitigate overshoot. Furthermore, we adapt this solution into a pre-computable form. Leveraging Cubature sampling, our runtime cost is only marginally higher than the Jacobi method, yet our algorithm converges nearly quadratically as Newton's method. We also introduce a novel full-coordinate formulation for more efficient pre-computation. Our method integrates seamlessly with the incremental potential contact method and achieves second-order convergence for both stiff and soft materials. Experimental results demonstrate that our approach delivers high-quality simulations and outperforms state-of-the-art GPU methods with 50\texttimes{} to 100\texttimes{} better convergence.},
journal = {ACM Trans. Graph. (SIGGRAPH)},
month = jul,
articleno = {44},
numpages = {15},
keywords = {GPU simulation, second-order jacobi method, newton's method, numerical optimization, parallel computation}
}

@article{Lu:2025:HPC,
author = {Lu, Zixuan and Liu, Ziheng and Lan, Lei and Wang, Huamin and Ishiwaka, Yuko and Jiang, Chenfanfu and Wu, Kui and Yang, Yin},
title = {High-performance {CPU} Cloth Simulation Using Domain-decomposed Projective Dynamics},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3731182},
doi = {10.1145/3731182},
abstract = {Whenever the concept of high-performance cloth simulation is brought up, GPU acceleration is almost always the first that comes to mind. Leveraging immense parallelization, GPU algorithms have demonstrated significant success recently, whereas CPU methods are somewhat overlooked. Indeed, the need for an efficient CPU simulator is evident and pressing. In many scenarios, high-end GPUs may be unavailable or are already allocated to other tasks, such as rendering and shading. A high-performance CPU alternative can greatly boost the overall system capability and user experience. Inspired by this demand, this paper proposes a CPU algorithm for high-resolution cloth simulation. By partitioning the garment model into multiple (but not massive) sub-meshes or domains, we assign per-domain computations to individual CPU processors. Borrowing the idea of projective dynamics that breaks the computation into global and local steps, our key contribution is a new parallelization paradigm at domains for both global and local steps so that domain-level calculations are sequential and lightweight. The CPU has much fewer processing units than a GPU. Our algorithm mitigates this disadvantage by wisely balancing the scale of the parallelization and convergence. We validate our method in a wide range of simulation problems involving high-resolution garment models. Performance-wise, our method is at least one order faster than existing CPU methods, and it delivers a similar performance compared with the state-of-the-art GPU algorithms in many examples, but without using a GPU.},
journal = {ACM Trans. Graph. (SIGGRAPH)},
month = jul,
articleno = {51},
numpages = {17},
keywords = {cloth simulation, parallel computation, GPU algorithm, CPU algorithm}
}

@inproceedings{Hu:2024:TPR,
author = {Hu, Rui and He, Qian and He, Gaofeng and Zhuang, Jiedong and Chen, Huang and Liu, Huafeng and Wang, Huamin},
title = {FashionR2R: texture-preserving rendered-to-real image translation with diffusion models},
year = {2024},
isbn = {9798331314385},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.},
booktitle = {Proceedings of the 38th International Conference on Neural Information Processing Systems},
articleno = {1107},
numpages = {30},
location = {Vancouver, BC, Canada},
series = {NIPS '24}
}

@inproceedings{Guo:2025:POA,
 abstract = {With the rise of digital fashion, reusing high-quality garment assets to assemble new outfits has become increasingly important for improving design efficiency and reducing production costs. However, combining multiple garments often introduces complex inter-garment intersections that are difficult to resolve. In this paper, we propose a novel framework that introduces a midsurface representation to simplify multilayered garments for intersection-free outfit assembly. Each garment is approximated by a watertight tetrahedral enclosure, enabling efficient resolution of inter-garment collisions on the midsurface level. To assemble an outfit, our method progressively untangles pairs of single-layer midsurfaces and incrementally constructs a merged midsurface. To recover the intersection-free full geometry from these deformed midsurfaces and enable instantaneous transfer across different poses, we uses embedded anchors to drive inversion-free deformation of enclosing tetrahedral cages. Through various examples, we demonstrate that our method provides a scalable and automated solution for virtual outfit coordination, enabling the direct reuse of garment assets in high-fidelity, collision-free digital fashion workflows.},
 address = {New York, NY, USA},
 author = {Dewen Guo and Zhendong Wang and Zegao Liu and Sheng Li and Guoping Wang and Yin Yang and Huamin Wang},
 booktitle = {SIGGRAPH Asia 2025 Conference Papers},
 doi = {10.1145/3757377.3763868},
 keywords = {cloth simulation, untangling, embedded deformation},
 location = {Hong Kong, China},
 month = {dec},
 numpages = {12},
 publisher = {Association for Computing Machinery},
 series = {SA Conference papers '25},
 title = {Progressive Outfit Assembly and Instantaneous Pose Transfer},
 url = {https://doi.org/10.1145/3757377.3763868},
 year = {2025}
}

@inproceedings{Gao:2025:EOR,
 abstract = {In 3D object reconstruction from photographs, estimating material properties is challenging.We propose an inverse rendering method that uses active area lighting: as this provides a wider range of lighting angles per photo than point lighting, material reconstruction can be more accurate for the same number of photos. We compare area light shading with point lighting. With either mesh or 3D Gaussian splatting pipelines, area lighting can improve BRDF reconstruction and leads to +3 dB relighting PSNR over point lights, or need only 1/5 of the input photos for the same quality. We also compare area light shading with Monte Carlo ray tracing and with differential linearly transformed cosines (LTC) plus shadow visibility weighting. LTC can be faster, improving optimization times by 25\%. In SOTA method-level comparisons, our approach improves material reconstruction, particularly for material roughness, leading to superior relighting quality.},
 address = {New York, NY, USA},
 author = {Yaoan Gao and Jiamin Xu and James Tompkin and Qi Wang and Zheng Dong and Hujun Bao and Yujun Shen and Huamin Wang and Changqing Zou and Weiwei Xu},
 booktitle = {SIGGRAPH Asia 2025 Conference Papers},
 doi = {10.1145/3757377.3763865},
 keywords = {inverse Rendering, point light, area light, linearly transformed cosines},
 location = {Hong Kong, China},
 month = {dec},
 numpages = {12},
 publisher = {Association for Computing Machinery},
 series = {SA Conference papers '25},
 title = {Efficient Object Reconstruction with Differentiable Area Light Shading},
 url = {https://doi.org/10.1145/3757377.3763865},
 year = {2025}
}

@article{Ma:2025:OSE,
author = {Jun Ma and Qian He and Gaofeng He and Huang Chen and Chen Liu and Xiaogang Jin and Huamin Wang},
title = {One-shot Embroidery Customization via Contrastive {LoRA} Modulation},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {6},
url = {https://doi.org/10.1145/3763290},
doi = {10.1145/3763290},
abstract = {Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer. Our project is available at: https://style3d.github.io/embroidery_customization.},
journal = {ACM Trans. Graph. (SIGGRAPH Asia)},
month = dec,
articleno = {271},
numpages = {15},
keywords = {embroidery customization, one-shot, low-rank adaptation, contrastive learning}
}

@article{Li:2025:GN,
author = {Siran Li and Ruiyang Liu and Chen Liu and Zhendong Wang and Gaofeng He and Yong-Lu Li and Xiaogang Jin and Huamin Wang},
title = {{GarmageNet}: {A} Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {6},
url = {https://doi.org/10.1145/3763271},
doi = {10.1145/3763271},
abstract = {Realistic digital garment modeling remains a labor-intensive task due to the intricate process of translating 2D sewing patterns into high-fidelity, simulation-ready 3D garments. We introduce GarmageNet, a unified generative framework that automates the creation of 2D sewing patterns, the construction of sewing relationships, and the synthesis of 3D garment initializations compatible with physics-based simulation. Central to our approach is Garmage, a novel garment representation that encodes each panel as a structured geometry image, effectively bridging the semantic and geometric gap between 2D structural patterns and 3D garment geometries. Followed by GarmageNet, a latent diffusion transformer to synthesize panel-wise geometry images and GarmageJigsaw, a neural module for predicting point-to-point sewing connections along panel contours. To support training and evaluation, we build GarmageSet, a large-scale dataset comprising 14,801 professionally designed garments with detailed structural and style annotations. Our method demonstrates versatility and efficacy across multiple application scenarios, including scalable garment generation from multi-modal design concepts (text prompts, sketches, photographs), automatic modeling from raw flat sewing patterns, pattern recovery from unstructured point clouds, and progressive garment editing using conventional instructions, laying the foundation for fully automated, production-ready pipelines in digital fashion. Refer to our project page for open-sourced code and dataset.},
journal = {ACM Trans. Graph. (SIGGRAPH Asia)},
month = dec,
articleno = {216},
numpages = {23},
keywords = {garment modeling, garment dataset, diffusion Generation}
}

@inproceedings{Zhou:2025:DGC,
 abstract = {Sewing patterns, the essential blueprints for fabric cutting and tailoring, act as a crucial bridge between design concepts and producible garments. However, existing uni-modal sewing pattern generation models struggle to effectively encode complex design concepts with a multi-modal nature and correlate them with vectorized sewing patterns that possess precise geometric structures and intricate sewing relations. In this work, we propose a novel sewing pattern generation approach Design2GarmentCode based on Large Multimodal Models (LMMs), to generate parametric pattern-making programs from multi-modal design concepts. LMM offers an intuitive interface for interpreting diverse design inputs, while pattern-making programs could serve as well-structured and semantically meaningful representations of sewing patterns, and act as a robust bridge connecting the cross-domain pattern-making knowledge embedded in LMMs with vectorized sewing patterns. Experimental results demonstrate that our method can flexibly handle various complex design expressions such as images, textual descriptions, designer sketches, or their combinations, and convert them into size-precise sewing patterns with correct stitches. Compared to previous methods, our approach significantly enhances training efficiency, generation quality, and authoring flexibility.},
 author = {Zhou, Feng and Liu, Ruiyang and Liu, Chen and He, Gaofeng and Li, Yong-Lu and Jin, Xiaogang and Wang, Huamin},
 booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 location = {Nashville, TN, USA},
 month = {jun},
 pages={23712-23722},
 series = {CVPR '25},
 title = {Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis},
 year = {2025}
}








